# ğŸ“ Handwritten Japanese Character Recognition â€” KMNIST Deep Learning Experiments

This repository contains my experiments and findings for handwritten Hiragana character recognition using the KMNIST dataset.  
I built and compared multiple fully connected neural network architectures to understand how model capacity and regularization affect performance.

---

## ğŸ“Š Dataset

- **Dataset:** KMNIST (70,000 grayscale 28Ã—28 images)
  - 60,000 training samples
  - 10,000 test samples
- **Classes:** 10 Hiragana characters
- **Preprocessing:** 
  - Added explicit channel dimension  
  - Normalized pixel values to [0, 1]  
  - Flattened images for fully connected layers  
  - Labels handled as integer indices and one-hot vectors as needed

---

## ğŸ§ª Experiments

### âœ… **Experiment 1 â€” Baseline Fully Connected Network**
- **Architecture:**
  - Input: 784-dim vector
  - Hidden Layers: [512 â†’ Dropout(0.2) â†’ 256 â†’ Dropout(0.2)]
  - Output: 10 raw logits
- **Training:**
  - Optimizer: Adam (lr=0.001)
  - Loss: CrossEntropyLoss
  - Epochs: 50, Batch Size: 128
- **Results:**
  - **Test Accuracy:** 92.54%  
  - **Observation:** Good baseline. Near-zero training loss shows overfitting.

---

### âœ… **Experiment 2 â€” Weight Decay Regularization**
- **Same architecture** as Experiment 1
- **Change:** Added L2 weight decay (`1e-5`)
- **Validation split:** 20% of training set
- **Results:**
  - **Validation Accuracy:** 98.56%  
  - **Test Accuracy:** 92.48%  
  - **Observation:** Weight decay improved generalization on validation data.

---

### âœ… **Experiment 3 â€” Increased Capacity + Higher Dropout**
- **Architecture:**
  - Hidden Layers: [1024 â†’ Dropout(0.5) â†’ 512 â†’ Dropout(0.5)]
- **Regularization:** Weight decay + higher dropout (0.5)
- **Results:**
  - **Validation Accuracy:** 96.78%  
  - **Test Accuracy:** 92.26%  
  - **Observation:** Larger model with aggressive dropout; slight drop in test performance â€” shows trade-off between capacity and over-regularization.

---

## ğŸ“ˆ Results Summary

| Experiment | Validation Accuracy | Test Accuracy |
|------------|---------------------|----------------|
| Exp 1      | â€”                   | **92.54%**     |
| Exp 2      | **98.56%**          | **92.48%**     |
| Exp 3      | **96.78%**          | **92.26%**     |

---

## ğŸ”‘ Key Takeaways

- **Baseline:** Simple networks can achieve solid results (~92%).
- **Regularization:** Weight decay helps generalization significantly.
- **Capacity vs Regularization:** Bigger networks need careful tuning of dropout and regularization to avoid under-utilizing model capacity.
- **Future Improvements:** Try CNNs, data augmentation, better hyperparameter tuning.

---
*Project Report (https://github.com/DrishyaChuke/Hiragana-Recognition-Neural-Network/blob/main/Report_PartB_25076922.pdf)
